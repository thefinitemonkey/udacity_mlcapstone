{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Machine Learning Capstone - Predict Movie Revenue\n",
    "\n",
    "This project works with the datasets from the Kaggle competition at https://www.kaggle.com/c/tmdb-box-office-prediction/data to create a model for predicting worldwide box office revenue for movies.\n",
    "\n",
    "There are 3000 movies in the sample data set being used for training and testing. The model created from this will then be used on a separate dataset to score the accuracy of predictions. The sample set contains 23 columns, of which 7 columns each contain JSON array data of objects. These objects, along with other fields, will require pre-processing into individual table columns for use in modeling. The fields the will be focuse on are:\n",
    "\n",
    "* **belongs_to_collection**: Indicates whether this movie is part of a series, and if so which series. Will be one-hot encoded as col_0 for none or col_X where X is the associated collection.\n",
    "* **budget**: Budget for film as an integer. Some entries have a budget of 0. It will be interesting to try the model both with and without this column as a consideration.\n",
    "* **genres**: Indicates the genres to which the film belongs. Will be one-hot encoded as gen_X where X is the associated genre.\n",
    "* **homepage**: Lists the URL for the homepage of the movie, if any. Will be encoded to 0 or 1 to indicate only whether the movie had a homepage.\n",
    "* **original_language**: Gives the ISO language value for the film. Will be one-hot encoded to lan_ISO where ISO is the ISO language value.\n",
    "* **popularity**: Long number value rating the film's popularity. This would not be a known value for future film productions, so it will be interesting to try modeling both with and without this value considered.\n",
    "* **production_companies**: Indicates production companies involved in the movie. Will be one-hot encoded as pcom_X where X is the id of the production company.\n",
    "* **production_countries**: Indicates countries where the movie was filmed or produced. Will be one-hot encoded as pcou_ISO where ISO is the ISO_3166_1 country value.\n",
    "* **release_date**: The release date for the film in mm/dd/yyyy format. Will be encoded to r_year and r_week columns, where r_week is the number of the week in the year in which the file was relesed (0-51)\n",
    "* **runtime**: Integer value for the runtime of the film in minutes.\n",
    "* **spoken_languages**: Indicates the languages spoken in the movie. Will be one-hot encoded to spo_ISO where ISO is the ISO_639-1 language value.\n",
    "* **Keywords**: Indicates the keyword values associated with the movie. Keyword objects have the format {'id': int, 'name': ''}, so these will be encoded to key_X where X is the keyword id.\n",
    "* **cast**: Indicates cast members associated with the movie. Will be one-hot encoded to cast_X where X is the id of the cast member.\n",
    "* **crew**: Indicates crew members associated with the movie. Will be one-hot encoded to crew_X where X is the id of the crew member.\n",
    "* **revenue**: Integer value for the worldwide revenue of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnized original_language shape: (3000, 52)\n",
      "Columnized crew shape: (3000, 2094)\n",
      "Exception doing literal eval on crew\n",
      "Columnized crew shape: (3000, 5648)\n",
      "Exception doing literal eval on crew\n",
      "Columnized crew shape: (3000, 7945)\n",
      "Columnized cast shape: (3000, 13991)\n",
      "Columnized genres shape: (3000, 14010)\n",
      "Columnized production_companies shape: (3000, 17721)\n",
      "Columnized production_countries shape: (3000, 17794)\n",
      "Columnized spoken_languages shape: (3000, 17872)\n",
      "Columnized Keywords shape: (3000, 25271)\n",
      "Movies dataset has 3000 data points with 25270 variables each.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import sys\n",
    "sys.path.insert(0, 'utilities')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from json_columnizer import jcolumnize\n",
    "from json_columnizer import crew_columnize\n",
    "from json_columnizer import columnBooleanize\n",
    "from json_columnizer import originalLanguage\n",
    "from fix_missing_data import fixTrainRevenueAndBudget\n",
    "from fix_missing_data import fixTrainRuntime\n",
    "from fix_missing_data import fixLowDollars\n",
    "from json_columnizer import columnizeDates\n",
    "\n",
    "# Import supplementary visualizations code visuals.py\n",
    "#import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Load the movies dataset\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Drop useless columns\n",
    "train = train.drop(['imdb_id', 'poster_path', 'original_title', 'overview', 'poster_path',\\\n",
    "                    'status', 'tagline', 'title'], axis = 1)\n",
    "\n",
    "# Fill missing revenue and budget numbers\n",
    "train = fixTrainRevenueAndBudget(train)\n",
    "\n",
    "# Fill missing runtime numbers\n",
    "train = fixTrainRuntime(train)\n",
    "\n",
    "# Fill dollar amounts represent in millions\n",
    "train = fixLowDollars(train)\n",
    "\n",
    "# Columnize dates\n",
    "train = columnizeDates(train, 'release_date')\n",
    "train = train.drop('release_date', axis=1)\n",
    "\n",
    "# Booleanize homepage column\n",
    "train = columnBooleanize(train, 'homepage')\n",
    "train = train.drop('homepage', axis=1)\n",
    "\n",
    "# Turn original language into columns\n",
    "train = originalLanguage(train, 'original_language', 'olang_')\n",
    "train = train.drop('original_language', axis=1)\n",
    "\n",
    "# Turn crew members into the appropriate columns\n",
    "# Director\n",
    "train = crew_columnize(train, 'crew', 'id', 'Director', 'director_')\n",
    "# Producer\n",
    "train = crew_columnize(train, 'crew', 'id', 'Producer', 'producer_')\n",
    "# Executive Producer\n",
    "train = crew_columnize(train, 'crew', 'id', 'Executive Producer', 'execprod_')\n",
    "train = train.drop('crew', axis=1)\n",
    "\n",
    "# Turn collections into booleans\n",
    "train = columnBooleanize(train, 'belongs_to_collection')\n",
    "train = train.drop('belongs_to_collection', axis=1)\n",
    "\n",
    "# Turn cast into columns\n",
    "train = jcolumnize(train, 'cast', 'id', 'cast_', 4)\n",
    "train = train.drop('cast', axis=1)\n",
    "\n",
    "# Turn genres into columns\n",
    "train = jcolumnize(train, 'genres', 'id', 'genres_')\n",
    "train = train.drop('genres', axis=1)\n",
    "\n",
    "# Turn production companies into columns\n",
    "train = jcolumnize(train, 'production_companies', 'id', 'pcomp_')\n",
    "train = train.drop('production_companies', axis=1)\n",
    "\n",
    "# Turn production countries into columns\n",
    "train = jcolumnize(train, 'production_countries', 'iso_3166_1', 'pcoun_')\n",
    "train = train.drop('production_countries', axis=1)\n",
    "\n",
    "# Turn spoken languages into columns\n",
    "train = jcolumnize(train, 'spoken_languages', 'iso_639_1', 'spoken_')\n",
    "train = train.drop('spoken_languages', axis=1)\n",
    "\n",
    "# Turn keywords into columns\n",
    "train = jcolumnize(train, 'Keywords', 'id', 'key_')\n",
    "train = train.drop('Keywords', axis=1)\n",
    "\n",
    "#print(train.head(3))\n",
    "\n",
    "# Success\n",
    "print(\"Movies dataset has {} data points with {} variables each.\".format(*train.shape))\n",
    "#print(\"Column names {}\".format(list(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_revenue = train['revenue']\n",
    "train_features = train.drop('revenue', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for dataset:\n",
      "\n",
      "Minimum revenue: $1\n",
      "Maximum revenue: $1519557910\n",
      "Mean revenue: $66724027.116333336\n",
      "Median revenue: $16928630.0\n",
      "Standard deviation of revenue: $137476234.10861793\n"
     ]
    }
   ],
   "source": [
    "# Explore some basic statistics regarding revenue\n",
    "train['revenue'] = train_revenue\n",
    "minimum_rev = np.amin(train_revenue)\n",
    "maximum_rev = np.amax(train_revenue)\n",
    "mean_rev = np.mean(train_revenue)\n",
    "median_rev = np.median(train_revenue)\n",
    "std_rev = np.std(train_revenue)\n",
    "\n",
    "print(\"Stats for dataset:\\n\")\n",
    "print(\"Minimum revenue: ${}\".format(minimum_rev))\n",
    "print(\"Maximum revenue: ${}\".format(maximum_rev))\n",
    "print(\"Mean revenue: ${}\".format(mean_rev))\n",
    "print(\"Median revenue: ${}\".format(median_rev))\n",
    "print(\"Standard deviation of revenue: ${}\".format(std_rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that data has been reformatted\n",
      "All original fields have been properly transformed and disposed\n",
      "Checking for bad value types\n",
      "There are 0 columns with suspect values\n",
      "Training and testing split successful\n"
     ]
    }
   ],
   "source": [
    "# Do final quality inspection and cleanup on the data\n",
    "print(\"Checking that data has been reformatted\")\n",
    "label_arr = ['id', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'imdb_id', 'original_language',\\\n",
    "            'original_title', 'overview', 'popularity', 'poster_path', 'production_companies',\\\n",
    "            'production_countries', 'release_date', 'runtime', 'spoken_languages', 'status', 'tagline',\\\n",
    "            'title', 'Keywords', 'cast', 'crew', 'revenue']\n",
    "count = 0\n",
    "for label in label_arr:\n",
    "    try:\n",
    "        print(\"Found {}: {}\".format(label, train_features.loc(0, label)))\n",
    "        count = count + 1\n",
    "    except:\n",
    "        count = count\n",
    "if count == 0:\n",
    "    print(\"All original fields have been properly transformed and disposed\")\n",
    "else:\n",
    "    print(\"All original fields have not been properly dealt with. See above.\")\n",
    "        \n",
    "print (\"Checking for bad value types\")\n",
    "count = 0\n",
    "columns = list(train_features.columns.values)\n",
    "for x in columns:\n",
    "    val = train_features.loc[0, x]\n",
    "    \n",
    "    if isinstance(val, str) or isinstance(val, list) or isinstance(val, dict) or isinstance(val, tuple):\n",
    "        print(\"Maybe bad column {} : {}\".format(x, val))\n",
    "        count = count + 1\n",
    "        train_features = train_features.drop(x, axis = 1)\n",
    "    else:\n",
    "        train_features.loc[train_features[x].isnull(), x] = 0\n",
    "\n",
    "    \n",
    "print(\"There are {} columns with suspect values\".format(count))\n",
    "\n",
    "# Shuffle and split the training set data to prepare for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_features, train_revenue, test_size=.2, random_state=42)\n",
    "print(\"Training and testing split successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doug/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/doug/anaconda3/lib/python3.7/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit sets: ShuffleSplit(3000, n_iter=10, test_size=0.2, random_state=0)\n",
      "Visualize training set sizes: [   1  301  600  900 1200 1500 1800 2099 2399]\n",
      "Evaluating depth 1\n",
      "Results for depth 1: [-0.46658877  0.29033064  0.32928962  0.31362965  0.3097127   0.30456712\n",
      "  0.29369417  0.29350156  0.29387471]\n",
      "Evaluating depth 3\n",
      "Results for depth 3: [-0.46658877  0.41747464  0.43050712  0.45518694  0.44331894  0.46081221\n",
      "  0.46953972  0.49456477  0.49517761]\n",
      "Evaluating depth 4\n",
      "Results for depth 4: [-0.46658877  0.41065712  0.41998532  0.44124461  0.44966434  0.42892392\n",
      "  0.41620699  0.46149406  0.48544788]\n",
      "Evaluating depth 5\n",
      "Results for depth 5: [-0.46658877  0.40616409  0.41658008  0.42323919  0.43480575  0.41236668\n",
      "  0.40059177  0.4602489   0.44426385]\n",
      "Evaluating depth 6\n",
      "Results for depth 6: [-0.46658877  0.37685592  0.42183512  0.39797306  0.41140013  0.42132189\n",
      "  0.42542183  0.47183684  0.43065621]\n",
      "Evaluating depth 10\n"
     ]
    }
   ],
   "source": [
    "import plots\n",
    "\n",
    "plots.VisualizeModelLearning(train_features, train_revenue)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root mean squared function\n",
    "def rmsle(y_true, y_predict):\n",
    "    sum = 0.0\n",
    "    for x in range(len(y_predict)):\n",
    "        if y_predict[x] < 0 or y_true.iloc[x] < 0:\n",
    "            continue\n",
    "        p = np.log(y_predict[x] + 1)\n",
    "        r = np.log(y_true.iloc[x] + 1)\n",
    "        sum = sum + (p - r) ** 2\n",
    "    return (sum / len(y_predict)) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def fit_model(X,y):\n",
    "    # Create cross-validation sets\n",
    "    cv_sets = ShuffleSplit(n_splits = 10, test_size = .2, train_size = None, random_state = None)\n",
    "    # Decision tree regressor\n",
    "    regressor = DecisionTreeRegressor()\n",
    "    # Dictionary for max_depth with a range of 1 to 10\n",
    "    params = {'max_depth': [30, 40, 50, 60, 80, 100], 'min_samples_split': [2, 4, 8, 16, 32, 64, 128], \\\n",
    "              'max_features': [10, 20, 40, 80, 100, 200, 400, 800, 1000]}\n",
    "    # Use rmsle as the scoring function\n",
    "    scoring_func = make_scorer(rmsle, greater_is_better=False)\n",
    "    \n",
    "    # Create grid search\n",
    "    grid = GridSearchCV(estimator = regressor, param_grid = params, scoring = scoring_func, cv = cv_sets)\n",
    "    \n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "    \n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = fit_model(X_train, y_train)\n",
    "\n",
    "# Check max_depth\n",
    "print(\"max_depth is {}\".format(reg.get_params()['max_depth']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the model and compare them to the real outcomes\n",
    "predicted = reg.predict(X_test)\n",
    "for x in range(20):\n",
    "    print(\"Predicted: {}        Real: {}\".format(int(predicted[x]), y_test.iloc[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the score for predicted vs actual\n",
    "score = rmsle(y_test, predicted)\n",
    "\n",
    "print(\"Final model score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check max_features and min_samples_split\n",
    "print(\"max_features is {}\".format(reg.get_params()['max_features']))\n",
    "print(\"min_samples_split is {}\".format(reg.get_params()['min_samples_split']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
